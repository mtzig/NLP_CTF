{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mtzig/NLP_CTF/blob/main/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpv2yPbX4pp8"
   },
   "source": [
    "### Baseline Toxicity Classifier\n",
    "Thomas Zeng\n",
    "9/27/22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KD---_ei4pp8"
   },
   "source": [
    "## Colab setup\n",
    "\n",
    "This section is only pertinent if the notebook is run in Colab and not on a local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLVsf9HN5haR"
   },
   "source": [
    "If you're using colab, make sure to run below code to clone the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FCygdiYr4sZb",
    "outputId": "568ff1c3-886f-45b8-8848-5dfe6d0de1be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'NLP_CTF'...\n",
      "remote: Enumerating objects: 162, done.\u001b[K\n",
      "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
      "remote: Total 162 (delta 19), reused 34 (delta 16), pack-reused 123\u001b[K\n",
      "Receiving objects: 100% (162/162), 102.56 MiB | 22.03 MiB/s, done.\n",
      "Resolving deltas: 100% (83/83), done.\n",
      "Updating files: 100% (29/29), done.\n",
      "[Errno 2] No such file or directory: '/content/NLP_CTF/'\n",
      "/Users/teaganjohnson/Desktop/NLP_CTF\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mtzig/NLP_CTF.git\n",
    "%cd /content/NLP_CTF/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cx6dyHemsq6E"
   },
   "source": [
    "Download Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uij1FLaRq8Ob",
    "outputId": "4c00cb62-9230-465f-a235-2b603309ac82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/teaganjohnson/Desktop/NLP_CTF/data\n",
      "--2022-10-03 13:34:43--  https://docs.google.com/uc?export=download&confirm=t&id=1JXm1N6SHmzIawgH7Aa4Ag-ZVuqLX7ba7\n",
      "Resolving docs.google.com (docs.google.com)... 142.250.190.14\n",
      "Connecting to docs.google.com (docs.google.com)|142.250.190.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-08-7o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8nsrvua6o5em6vg7nkfu6fcndcg453g0/1664822025000/15857340408018396550/*/1JXm1N6SHmzIawgH7Aa4Ag-ZVuqLX7ba7?e=download&uuid=4e8c9954-c006-4d8a-b5ea-36a2026a262c [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-10-03 13:34:43--  https://doc-08-7o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8nsrvua6o5em6vg7nkfu6fcndcg453g0/1664822025000/15857340408018396550/*/1JXm1N6SHmzIawgH7Aa4Ag-ZVuqLX7ba7?e=download&uuid=4e8c9954-c006-4d8a-b5ea-36a2026a262c\n",
      "Resolving doc-08-7o-docs.googleusercontent.com (doc-08-7o-docs.googleusercontent.com)... 142.250.191.129\n",
      "Connecting to doc-08-7o-docs.googleusercontent.com (doc-08-7o-docs.googleusercontent.com)|142.250.191.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3644258522 (3.4G) [application/macbinary]\n",
      "Saving to: ‘GoogleNews-vectors-negative300.bin’\n",
      "\n",
      "GoogleNews-vectors- 100%[===================>]   3.39G  23.7MB/s    in 2m 20s  \n",
      "\n",
      "2022-10-03 13:37:04 (24.8 MB/s) - ‘GoogleNews-vectors-negative300.bin’ saved [3644258522/3644258522]\n",
      "\n",
      "/Users/teaganjohnson/Desktop/NLP_CTF\n"
     ]
    }
   ],
   "source": [
    "%cd ./data\n",
    "# !wget -O 'GoogleNews-vectors-negative300.bin.gz' 'https://drive.google.com/u/0/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download&confirm=t&uuid=e1f49911-ab4d-44ba-af00-f6733ccabb98'\n",
    "# !gzip -d 'GoogleNews-vectors-negative300.bin.gz'\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1JXm1N6SHmzIawgH7Aa4Ag-ZVuqLX7ba7' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1JXm1N6SHmzIawgH7Aa4Ag-ZVuqLX7ba7\" -O GoogleNews-vectors-negative300.bin && rm -rf /tmp/cookies.txt\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8VF2PxN6OM-"
   },
   "source": [
    "Colab does not have the Python library `transformers` (which I use in below code) automatically installed, so we meed to manually install when we start up instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVZwKqJ5q3D5",
    "outputId": "be602512-4ed8-48af-9d00-2b5c0b40a79c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /Users/teaganjohnson/Library/Python/3.8/lib/python/site-packages (from transformers) (2020.10.28)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (20.4)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-macosx_10_11_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (1.19.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (5.3.1)\n",
      "Collecting huggingface-hub<1.0,>=0.9.0\n",
      "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->transformers) (2020.6.20)\n",
      "Installing collected packages: tokenizers, packaging, huggingface-hub, transformers\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.4\n",
      "    Uninstalling packaging-20.4:\n",
      "      Successfully uninstalled packaging-20.4\n",
      "Successfully installed huggingface-hub-0.10.0 packaging-21.3 tokenizers-0.12.1 transformers-4.22.2\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting gensim\n",
      "  Downloading gensim-4.2.0-cp38-cp38-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gensim) (1.19.4)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.2.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gensim) (1.5.2)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.2.0 smart-open-6.2.0\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install --upgrade gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSKuhXzx57uc"
   },
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "3YIa35bp4pp8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from process_data import get_jigsaw_datasets, init_embed_lookup\n",
    "from models import CNNClassifier\n",
    "from train_eval import train, evaluate\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CewFs_b84pp9"
   },
   "source": [
    "## Data Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cg_HhJyT4pp9"
   },
   "source": [
    "Pytorch requires its datasets to be ascessible following the [datasets api](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files).\n",
    "\n",
    "Below I wrote a simple function to load in the [Jigsaw Dataset](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) that the paper [Counterfactual Fairness in\n",
    "Text Classification through Robustness](https://dl.acm.org/doi/pdf/10.1145/3306618.3317950) used to train its toxicity classifier.\n",
    "\n",
    "I use only a very small subset of the available data here for demonstration purposes. Specificaly 256 comments (128 toxic and 128 nontoxic) sampled randomly for the train set and test set respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "vz8b315r4pp-"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = get_jigsaw_datasets(device=DEVICE, data_set='blind') #demo_mode=True only loads a subset of entire dataset so as to make training faster for demonstration purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9S_0KL-84pp-"
   },
   "source": [
    "PyTorch models receive data for training and inference through a dataloader. A dataloader samples from a dataset and returns a batch of samples each time it is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Ka5Bi4Kj4pp-"
   },
   "outputs": [],
   "source": [
    "train_loader =  torch.utils.data.DataLoader(train_data, batch_size=64)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNm5htHo4pp-"
   },
   "source": [
    "## Model and Training Stuff Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "DLBsb2OXq3D6"
   },
   "outputs": [],
   "source": [
    "pretrained_embed = torch.from_numpy(init_embed_lookup().vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "jK6qZ1854pp-"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = CNNClassifier(pretrained_embed,device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaQxgG1g4pp-"
   },
   "source": [
    "An epoch is the number of times you go through your datase during training. That is you have trained for 1 epoch when you have seen every sample in your training dataset once.<br>\n",
    "The loss function is the training objective we want our model to minimize.<br>\n",
    "The optimizer is used at every time step i.e. everyime we compute the loss and its gradient. It is used to update the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "HbLvNOKd4pp-"
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ce6OPZb4pp-"
   },
   "source": [
    "## Train and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qrcbly74pp-"
   },
   "source": [
    "For traing, we train for 10 epochs. <br>\n",
    "In general, you should (or more specifically are required to) train and evaluate using different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Jg1pkhd4pp-",
    "outputId": "65a4826d-e515-490f-c73d-73a1e930acf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2494/2494 [04:55<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.1266589334959787\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 103/2494 [00:12<04:42,  8.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     train(train_loader, model, loss_fn, optimizer, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/repos/NLP_CTF/train_eval.py:26\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer, verbose)\u001b[0m\n\u001b[1;32m     21\u001b[0m num_batches \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader)\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m _, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(data_iter)):\n\u001b[0;32m---> 26\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(model(X), y)\n\u001b[1;32m     27\u001b[0m     avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     29\u001b[0m     \u001b[39m# Backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/NLP_CTF/models.py:91\u001b[0m, in \u001b[0;36mCNNClassifier.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     88\u001b[0m features \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(features)\n\u001b[1;32m     90\u001b[0m \u001b[39m# dropout\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(features)\n\u001b[1;32m     93\u001b[0m \u001b[39m# fully connected layer\u001b[39;00m\n\u001b[1;32m     94\u001b[0m logit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(features)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL/lib/python3.10/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    train(train_loader, model, loss_fn, optimizer, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXIHJ9_9RSUI"
   },
   "source": [
    "We first evaluate our results on our train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0MyzVR_RAd0",
    "outputId": "e26d16af-b63b-474a-b06c-c65b83a79d0a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2494/2494 [01:27<00:00, 28.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.09106984734535217\n",
      "Accuracy: 0.9655764518615538, Sensitivity: 0.7589904537727213, Specificity: 0.9874754811924285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate(train_loader, model, get_loss=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjFQ64q54pp-"
   },
   "source": [
    "We then evaluate our reults on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9u43Jv2c4pp-",
    "outputId": "7491a9b5-3538-47f0-8c26-6a5d9d4bb99c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:35<00:00, 27.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.18660908937454224\n",
      "Accuracy: 0.9213948544812279, Sensitivity: 0.813136288998358, Specificity: 0.9327839966832504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate(test_loader, model, get_loss=True, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b5da4dccc5d959110f70bd428b51197bb0688003461a0e87be372a9c01e32ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
