{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import CNNClassifier\n",
    "from getData import init_embed_lookup\n",
    "from datasets import SimpleDataset\n",
    "from dataloaders import InfiniteDataLoader\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y, A\n",
    "# X (10, 3 ,2)\n",
    "# A (10, 5, 3,2)\n",
    "# y (10. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  model, dataloader\n",
    "#\n",
    "#\n",
    "#  steps_per_epoch = dataloader.batches_per_epoch()\n",
    "#\n",
    "#  for each batch in the dataloder:\n",
    "# X, A, _ = batch\n",
    "# X_pred = softmax(model(X))\n",
    "# A_pred = softmax(model(A.reshape))).reshape\n",
    "# X_pred.unsqueeze(1) - A_pred\n",
    "#\n",
    "# normalize\n",
    "#\n",
    "#\n",
    "\n",
    "def CTF(dataloader, model):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    data_iter = iter(dataloader)\n",
    "    \n",
    "    cum_gap = 0\n",
    "    num_examples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _, (X,A) in enumerate(data_iter):\n",
    "\n",
    "            # this is redundant to do every iteration, but whatever\n",
    "            l, i ,w = A.shape\n",
    "            \n",
    "            A_preds = torch.nn.functional.softmax(model(A.reshape(-1, w)), 1).reshape(l, i, -1)[:,:,0]\n",
    "            X_preds = torch.unsqueeze(torch.nn.functional.softmax(model(X), 1)[:,0], 1)\n",
    "\n",
    "            cum_gap += torch.sum(torch.abs(X_preds - A_preds))\n",
    "\n",
    "            num_examples += l * i\n",
    "\n",
    "    return cum_gap / num_examples\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.ones(100,50).long()\n",
    "A = torch.ones(100, 50, 50).long()\n",
    "\n",
    "data = DataLoader(SimpleDataset(X,A), batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNClassifier(torch.from_numpy(init_embed_lookup().vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CTF(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, w = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preds = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5384, 0.2383],\n",
       "          [0.6750, 0.9454],\n",
       "          [0.3480, 0.6078]],\n",
       "\n",
       "         [[0.3930, 0.7666],\n",
       "          [0.0625, 0.1206],\n",
       "          [0.8139, 0.1453]],\n",
       "\n",
       "         [[0.9737, 0.4834],\n",
       "          [0.5543, 0.5858],\n",
       "          [0.6492, 0.9314]],\n",
       "\n",
       "         [[0.5005, 0.3915],\n",
       "          [0.7039, 0.8161],\n",
       "          [0.1176, 0.6174]],\n",
       "\n",
       "         [[0.7929, 0.3162],\n",
       "          [0.1610, 0.0496],\n",
       "          [0.5284, 0.5418]]],\n",
       "\n",
       "\n",
       "        [[[0.4199, 0.0546],\n",
       "          [0.5261, 0.1512],\n",
       "          [0.6387, 0.3455]],\n",
       "\n",
       "         [[0.2906, 0.2175],\n",
       "          [0.0537, 0.0855],\n",
       "          [0.0704, 0.3598]],\n",
       "\n",
       "         [[0.4880, 0.4343],\n",
       "          [0.1231, 0.5712],\n",
       "          [0.0313, 0.9920]],\n",
       "\n",
       "         [[0.3715, 0.6189],\n",
       "          [0.2086, 0.7899],\n",
       "          [0.9462, 0.4272]],\n",
       "\n",
       "         [[0.4950, 0.2037],\n",
       "          [0.0447, 0.6420],\n",
       "          [0.9772, 0.7601]]],\n",
       "\n",
       "\n",
       "        [[[0.3583, 0.4031],\n",
       "          [0.6528, 0.1935],\n",
       "          [0.1119, 0.8351]],\n",
       "\n",
       "         [[0.5086, 0.8411],\n",
       "          [0.5394, 0.0990],\n",
       "          [0.6820, 0.4043]],\n",
       "\n",
       "         [[0.3368, 0.5874],\n",
       "          [0.6104, 0.1455],\n",
       "          [0.2160, 0.0751]],\n",
       "\n",
       "         [[0.3923, 0.2068],\n",
       "          [0.7949, 0.7115],\n",
       "          [0.9800, 0.5184]],\n",
       "\n",
       "         [[0.6039, 0.2020],\n",
       "          [0.0939, 0.5258],\n",
       "          [0.3037, 0.3324]]],\n",
       "\n",
       "\n",
       "        [[[0.4452, 0.7694],\n",
       "          [0.3180, 0.3142],\n",
       "          [0.7421, 0.1491]],\n",
       "\n",
       "         [[0.7856, 0.1484],\n",
       "          [0.2220, 0.1293],\n",
       "          [0.1059, 0.1897]],\n",
       "\n",
       "         [[0.6237, 0.9271],\n",
       "          [0.0230, 0.0365],\n",
       "          [0.4020, 0.8901]],\n",
       "\n",
       "         [[0.6195, 0.8028],\n",
       "          [0.2967, 0.1771],\n",
       "          [0.3628, 0.5793]],\n",
       "\n",
       "         [[0.2851, 0.3547],\n",
       "          [0.1773, 0.1066],\n",
       "          [0.4489, 0.9870]]],\n",
       "\n",
       "\n",
       "        [[[0.4838, 0.9359],\n",
       "          [0.4026, 0.0214],\n",
       "          [0.9470, 0.2393]],\n",
       "\n",
       "         [[0.8049, 0.8867],\n",
       "          [0.9220, 0.5848],\n",
       "          [0.9428, 0.3791]],\n",
       "\n",
       "         [[0.0134, 0.3976],\n",
       "          [0.0827, 0.1886],\n",
       "          [0.1714, 0.4244]],\n",
       "\n",
       "         [[0.2820, 0.0500],\n",
       "          [0.9025, 0.2193],\n",
       "          [0.3102, 0.2175]],\n",
       "\n",
       "         [[0.8321, 0.4231],\n",
       "          [0.9041, 0.2275],\n",
       "          [0.8871, 0.9499]]],\n",
       "\n",
       "\n",
       "        [[[0.2704, 0.7406],\n",
       "          [0.6240, 0.7774],\n",
       "          [0.1195, 0.0733]],\n",
       "\n",
       "         [[0.6075, 0.4995],\n",
       "          [0.1125, 0.1153],\n",
       "          [0.8564, 0.5875]],\n",
       "\n",
       "         [[0.3163, 0.4800],\n",
       "          [0.5776, 0.3969],\n",
       "          [0.7176, 0.9656]],\n",
       "\n",
       "         [[0.6458, 0.9008],\n",
       "          [0.2511, 0.8919],\n",
       "          [0.1029, 0.4654]],\n",
       "\n",
       "         [[0.4999, 0.9575],\n",
       "          [0.5597, 0.0288],\n",
       "          [0.7760, 0.6151]]],\n",
       "\n",
       "\n",
       "        [[[0.4170, 0.5080],\n",
       "          [0.7655, 0.1516],\n",
       "          [0.5683, 0.6094]],\n",
       "\n",
       "         [[0.5146, 0.8114],\n",
       "          [0.7493, 0.8743],\n",
       "          [0.7825, 0.9534]],\n",
       "\n",
       "         [[0.3156, 0.3926],\n",
       "          [0.9833, 0.6532],\n",
       "          [0.8256, 0.6802]],\n",
       "\n",
       "         [[0.2014, 0.2331],\n",
       "          [0.5537, 0.2128],\n",
       "          [0.0375, 0.6202]],\n",
       "\n",
       "         [[0.7842, 0.7941],\n",
       "          [0.7871, 0.9018],\n",
       "          [0.0736, 0.6349]]],\n",
       "\n",
       "\n",
       "        [[[0.5519, 0.0382],\n",
       "          [0.8978, 0.1094],\n",
       "          [0.2940, 0.3354]],\n",
       "\n",
       "         [[0.9192, 0.1610],\n",
       "          [0.5072, 0.0264],\n",
       "          [0.1531, 0.6179]],\n",
       "\n",
       "         [[0.4756, 0.4014],\n",
       "          [0.9315, 0.4092],\n",
       "          [0.7937, 0.7431]],\n",
       "\n",
       "         [[0.5255, 0.8091],\n",
       "          [0.4446, 0.4594],\n",
       "          [0.9732, 0.4278]],\n",
       "\n",
       "         [[0.9570, 0.1615],\n",
       "          [0.5792, 0.3800],\n",
       "          [0.3842, 0.1883]]],\n",
       "\n",
       "\n",
       "        [[[0.0402, 0.3230],\n",
       "          [0.4788, 0.6004],\n",
       "          [0.3279, 0.9287]],\n",
       "\n",
       "         [[0.0262, 0.3912],\n",
       "          [0.5875, 0.9323],\n",
       "          [0.2197, 0.0716]],\n",
       "\n",
       "         [[0.0883, 0.5368],\n",
       "          [0.6536, 0.3227],\n",
       "          [0.0599, 0.9489]],\n",
       "\n",
       "         [[0.6619, 0.2209],\n",
       "          [0.6027, 0.7862],\n",
       "          [0.8953, 0.0056]],\n",
       "\n",
       "         [[0.6915, 0.5171],\n",
       "          [0.9846, 0.8417],\n",
       "          [0.2119, 0.2225]]],\n",
       "\n",
       "\n",
       "        [[[0.7580, 0.8030],\n",
       "          [0.1241, 0.5054],\n",
       "          [0.9416, 0.8714]],\n",
       "\n",
       "         [[0.1418, 0.1233],\n",
       "          [0.0341, 0.0115],\n",
       "          [0.1328, 0.0181]],\n",
       "\n",
       "         [[0.7979, 0.5080],\n",
       "          [0.0636, 0.8359],\n",
       "          [0.7193, 0.1262]],\n",
       "\n",
       "         [[0.9081, 0.7247],\n",
       "          [0.2336, 0.3161],\n",
       "          [0.9858, 0.2566]],\n",
       "\n",
       "         [[0.8484, 0.2886],\n",
       "          [0.4660, 0.7809],\n",
       "          [0.7963, 0.4207]]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_preds = model(A.reshape(-1, w, d)).reshape(l, -1, w, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b5da4dccc5d959110f70bd428b51197bb0688003461a0e87be372a9c01e32ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
