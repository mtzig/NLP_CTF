{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "\n",
    "def get_identities(train_path, test_path):\n",
    "    '''\n",
    "        input: path to full identity list\n",
    "        output: returns 2 random identity lists: training and test\n",
    "    '''\n",
    "    train_identities_raw = pd.read_csv(train_path)\n",
    "    test_identities = pd.read_csv(test_path)\n",
    "    \n",
    "    train_identities_listed = train_identities_raw.to_numpy().tolist()\n",
    "\n",
    "    train_identities = []\n",
    "    for i in train_identities_listed:\n",
    "        train_identities.append(i[0])\n",
    "        \n",
    "    return train_identities, test_identities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(toxic, path, identity_list):\n",
    "    '''\n",
    "    input: \n",
    "        boolean toxic (true if we want toxic data)\n",
    "        path to data set\n",
    "        identity list we want to use (test or train)\n",
    "    output: returns modified dataframe\n",
    "    \n",
    "    This is specific to synthetic data sets (very small difference with \n",
    "    original datasets labels and toxicity measurements).\n",
    "    '''\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    sentences = []\n",
    "    a = []\n",
    "    toxicity = []\n",
    "\n",
    "    for row_index in tqdm(range(len(df))):\n",
    "        comment_text = df.iloc[row_index]['Text'].split()\n",
    "        if toxic:\n",
    "            if df.iloc[row_index]['Label'] == \"BAD\" and len(set(identity_list).intersection(comment_text)) != 0:\n",
    "                identity = str(set(identity_list).intersection(comment_text).pop())\n",
    "                sentences.append(df.iloc[row_index]['Text'])\n",
    "                toxicity.append(0)\n",
    "                cur_a = []\n",
    "\n",
    "                for diff_identity in identity_list:\n",
    "                    cur_a.append(df.at[row_index, \"Text\"].replace(identity, diff_identity))\n",
    "                a.append(cur_a)\n",
    "        else:\n",
    "            if df.iloc[row_index]['Label'] == \"NOT_BAD\" and len(set(identity_list).intersection(comment_text)) != 0:\n",
    "                identity = str(set(identity_list).intersection(comment_text).pop())\n",
    "                sentences.append(df.iloc[row_index]['Text'])\n",
    "                toxicity.append(0)\n",
    "                cur_a = []\n",
    "\n",
    "                for diff_identity in identity_list:\n",
    "                    cur_a.append(df.at[row_index, \"Text\"].replace(identity, diff_identity))\n",
    "                a.append(cur_a)\n",
    "\n",
    "    return_df_raw = pd.DataFrame(list(zip(*a)))\n",
    "    return_df = return_df_raw.T\n",
    "    return_df.insert(0, column='comment_text', value=sentences)\n",
    "\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    train_ids, test_ids = get_identities(\"../data/train_identities.txt\", \"../data/test_identities.txt\")\n",
    "\n",
    "    synthetic_toxic_1 = generate_synthetic_data(True, \"../data/bias_madlibs_89k.csv\", train_ids)\n",
    "    synthetic_nontoxic_1 = generate_synthetic_data(False, \"../data/bias_madlibs_89k.csv\", train_ids)\n",
    "    \n",
    "    synthetic_toxic_2 = generate_synthetic_data(True, \"../data/bias_madlibs_77k.csv\", train_ids)\n",
    "    synthetic_nontoxic_2 = generate_synthetic_data(False, \"../data/bias_madlibs_77k.csv\", train_ids)\n",
    "    \n",
    "    synthetic_toxic_1.to_csv(Path(\"../data/synthetic/synthetic_toxic_df_1.csv\"))\n",
    "    synthetic_nontoxic_1.to_csv(Path(\"../data/synthetic/synthetic_nontoxic_df_1.csv\"))\n",
    "    \n",
    "    synthetic_toxic_2.to_csv(Path(\"../data/synthetic/synthetic_toxic_df_2.csv\"))\n",
    "    synthetic_nontoxic_2.to_csv(Path(\"../data/synthetic/synthetic_nontoxic_df_2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89483/89483 [00:20<00:00, 4282.64it/s]\n",
      "100%|██████████| 89483/89483 [00:20<00:00, 4360.88it/s]\n",
      "100%|██████████| 76564/76564 [00:17<00:00, 4396.20it/s]\n",
      "100%|██████████| 76564/76564 [00:17<00:00, 4273.89it/s]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
